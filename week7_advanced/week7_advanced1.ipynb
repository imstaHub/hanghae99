{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week7 Advanced1 Homework\n",
    "- ChatGPT의 MapReduce를 모방한 요약 corpus 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, List, TypedDict\n",
    "\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "## MapReduce class and function\n",
    "class OverallState(TypedDict):\n",
    "    # Notice here we use the operator.add\n",
    "    # This is because we want combine all the summaries we generate\n",
    "    # from individual nodes back into one list - this is essentially\n",
    "    # the \"reduce\" part\n",
    "    contents: List[str]\n",
    "    summaries: Annotated[list, operator.add]\n",
    "    final_summary: str\n",
    "\n",
    "\n",
    "# This will be the state of the node that we will \"map\" all\n",
    "# documents to in order to generate summaries\n",
    "class SummaryState(TypedDict):\n",
    "    content: str\n",
    "\n",
    "\n",
    "# Here we generate a summary, given a document\n",
    "async def generate_summary(state: SummaryState):\n",
    "    response = await map_chain.ainvoke(state[\"content\"])\n",
    "    return {\"summaries\": [response]}\n",
    "\n",
    "\n",
    "# Here we define the logic to map out over the documents\n",
    "# We will use this an edge in the graph\n",
    "def map_summaries(state: OverallState):\n",
    "    # We will return a list of `Send` objects\n",
    "    # Each `Send` object consists of the name of a node in the graph\n",
    "    # as well as the state to send to that node\n",
    "    return [\n",
    "        Send(\"generate_summary\", {\"content\": content}) for content in state[\"contents\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "# Here we will generate the final summary\n",
    "async def generate_final_summary(state: OverallState):\n",
    "    response = await reduce_chain.ainvoke(state[\"summaries\"])\n",
    "    return {\"final_summary\": response}\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pprint\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MY CODE] 데이터셋 만들기\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dl paper.pdf', 'Artical.pdf', 'deep neural network.pdf', '.DS_Store', 'bayesian deep learning.pdf', 'RAG_paper.pdf', 'An Improved Particle Filter.pdf', 'NIPS-2017-attention.pdf', 'deep learning.pdf']\n"
     ]
    }
   ],
   "source": [
    "file_list = os.listdir('week7_pdf')\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map prompt\n",
    "map_template = \"\"\"This is a part of document:\n",
    "{pages}\n",
    "\n",
    "Please summarize the main points of the content.\n",
    "Answer:\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# Reduce prompt\n",
    "reduce_template = \"\"\"This is a set of summary:\n",
    "{doc_summaries}\n",
    "\n",
    "Please write a comprehensive summary of this..\n",
    "Answer: \"\"\"\n",
    "\n",
    "# Reduce 프롬프트 완성\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0,\n",
    "                 model_name='gpt-4o-mini',\n",
    "                 api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 2 65536 (offset 0)\n",
      "Ignoring wrong pointing object 34 65536 (offset 0)\n",
      "Ignoring wrong pointing object 92 65536 (offset 0)\n",
      "Ignoring wrong pointing object 145 65536 (offset 0)\n",
      "Ignoring wrong pointing object 206 65536 (offset 0)\n",
      "Ignoring wrong pointing object 274 65536 (offset 0)\n",
      "Ignoring wrong pointing object 330 65536 (offset 0)\n",
      "Ignoring wrong pointing object 372 65536 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl paper.pdf | doc 수: 8 | split 수: 12\n",
      "corpus count: 13\n",
      "Artical.pdf | doc 수: 16 | split 수: 13\n",
      "corpus count: 27\n",
      "deep neural network.pdf | doc 수: 9 | split 수: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'\\x00\\x00\\x00\\x01B'\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus count: 36\n",
      "bayesian deep learning.pdf | doc 수: 7 | split 수: 4\n",
      "corpus count: 41\n",
      "RAG_paper.pdf | doc 수: 19 | split 수: 15\n",
      "corpus count: 57\n",
      "An Improved Particle Filter.pdf | doc 수: 14 | split 수: 14\n",
      "corpus count: 72\n",
      "NIPS-2017-attention.pdf | doc 수: 11 | split 수: 10\n",
      "corpus count: 83\n",
      "deep learning.pdf | doc 수: 11 | split 수: 12\n",
      "corpus count: 96\n"
     ]
    }
   ],
   "source": [
    "corpus_set = []\n",
    "for file_name in file_list:\n",
    "    try:\n",
    "        loader = PyPDFLoader(\n",
    "            file_path = 'week7_pdf/'+file_name,\n",
    "        )\n",
    "        docs = loader.load()\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    page_contents = []\n",
    "    for doc in docs:\n",
    "        tmp = doc.page_content\n",
    "        if('References' in tmp):\n",
    "            tmp = tmp.split('References\\n')[0]\n",
    "            page_contents.append(tmp)\n",
    "            break\n",
    "        elif('references' in tmp):\n",
    "            tmp = tmp.split('references\\n')[0]\n",
    "            page_contents.append(tmp)\n",
    "            break\n",
    "        else:\n",
    "            page_contents.append(tmp)\n",
    "\n",
    "    page_contents = '\\n'.join(page_contents)\n",
    "    page_contents = page_contents\n",
    "\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",  # 분할기준\n",
    "        chunk_size=900,   # 사이즈\n",
    "        chunk_overlap=200, # 중첩 사이즈\n",
    "    )\n",
    "\n",
    "    # 분할 실행\n",
    "    split_docs = text_splitter.split_text(page_contents)\n",
    "    split_docs = [split.replace('-\\n','-').replace('\\n',' ') for split in split_docs]\n",
    "\n",
    "    print(file_name, '| doc 수:', len(docs), '| split 수:', len(split_docs))\n",
    "\n",
    "\n",
    "    ### Map Reduce\n",
    "    map_prompt = ChatPromptTemplate([(\"human\", map_template)])\n",
    "    map_chain = map_prompt | llm | StrOutputParser()\n",
    "\n",
    "    reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
    "    reduce_chain = reduce_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Construct the graph: here we put everything together to construct our graph\n",
    "    graph = StateGraph(OverallState)\n",
    "    graph.add_node(\"generate_summary\", generate_summary)\n",
    "    graph.add_node(\"generate_final_summary\", generate_final_summary)\n",
    "    graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])\n",
    "    graph.add_edge(\"generate_summary\", \"generate_final_summary\")\n",
    "    graph.add_edge(\"generate_final_summary\", END)\n",
    "    app = graph.compile()\n",
    "\n",
    "    rsts = []\n",
    "    async for step in app.astream({\"contents\": split_docs}):\n",
    "        rsts.append(step)\n",
    "        #print(list(step.keys()))\n",
    "\n",
    "\n",
    "    # GPT 결과와 map prompt, reduce prompt를 통해 corpus 데이터셋 만들기\n",
    "    for i in range(len(rsts)):\n",
    "        if('generate_summary' in rsts[i].keys()):\n",
    "            map_content = rsts[i]['generate_summary']['summaries']\n",
    "            if(not isinstance(map,str)):\n",
    "                map_content = map_content[0]\n",
    "            corpus = {\"input\": map_prompt.format(pages=split_docs[i]),\n",
    "                    \"output\": map_content}\n",
    "\n",
    "\n",
    "        else:\n",
    "            maps = [rst['generate_summary']['summaries'][0] for rst in rsts[:-1]]\n",
    "            maps_content = '\\n'.join(maps)\n",
    "            reduce_content = rsts[i]['generate_final_summary']['final_summary']\n",
    "            corpus = {\"input\": reduce_prompt.format(doc_summaries=maps_content),\n",
    "                      \"output\": reduce_content}\n",
    "\n",
    "        corpus_set.append(corpus)\n",
    "\n",
    "    print('corpus count:', len(corpus_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus final count: 96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'input': 'Human: This is a part of document:\\n249 Understanding the difﬁculty of training deep feedforward neural networks Xavier Glorot Yoshua Bengio DIRO, Universit´e de Montr ´eal, Montr ´eal, Qu ´ebec, Canada Abstract Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experi-mental results showing the superiority of deeper vs less deep architectures. All these experimen-tal results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations func-tions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top hidden layer into satu-ration. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during train-ing, with the idea that training may be more dif-ﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 2010, Chia La-guna Resort, Sardinia, Italy. V olume 9 of JMLR: W&CP 9. Copy-right 2010 by the authors. learning methods for a wide array of deep architectures, including neural networks with many hidden layers (Vin-cent et al., 2008) and graphical models with many levels of hidden variables (Hinton et al., 2006), among others (Zhu et al., 2009; Weston et al., 2008). Much attention has re-cently been devoted to them (see (Bengio, 2009) for a re-view), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language process-ing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Ben-gio (2009), suggest that in order to learn the kind of com-plicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep archi-tecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pre-training (Erhan et al., 2009), showing that it acts as a regu-larizer that initializes the parameters in a “better” basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better general-\\n\\nPlease summarize the main points of the content.\\nAnswer:',\n",
       "  'output': 'The document discusses challenges in training deep feedforward neural networks, highlighting key findings from experiments on different datasets. The main points include:\\n\\n1. **Activation Functions and Initialization**: The choice of activation functions and weight initialization schemes significantly impacts training performance, with specific configurations leading to lower test errors.\\n\\n2. **Avoiding Sigmoid Activations**: Sigmoid activations that are not symmetric around zero should be avoided when using small random weights, as they can lead to poor learning dynamics and saturation in the top hidden layer.\\n\\n3. **Layer-to-Layer Transformations**: Maintaining effective transformations between layers that ensure good flow of activations and gradients (ideally with a Jacobian around 1) is beneficial. This approach helps bridge the gap between supervised and unsupervised learning methods.\\n\\n4. **Need for Further Research**: Many observations regarding gradients and training dynamics remain unexplained, indicating a need for further investigation to enhance understanding of deep network training.'},\n",
       " {'input': 'Human: This is a part of document:\\nplicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep archi-tecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pre-training (Erhan et al., 2009), showing that it acts as a regu-larizer that initializes the parameters in a “better” basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better general-ization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise proce-dure would give better results. So here instead of focus-ing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multi-layer neural networks. Our analysis is driven by investigative experiments to mon-itor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of acti-vation function (with the idea that it might affect satura-tion) and initialization procedure (since unsupervised pre-training is a particular form of initialization and it has a drastic impact).          250 Understanding the difﬁculty of training deep feedforward neural networks 2 Experimental Setting and Datasets Code to produce the new datasets introduced in this section is available from: http://www.iro.umontreal. ca/˜lisa/twiki/bin/view.cgi/Public/ DeepGradientsAISTATS2010. 2.1 Online Learning on an Inﬁnite Dataset: Shapeset-3 ×2 Recent work with deep architectures (see Figure 7 in Ben-gio (2009)) shows that even with very large training sets or online learning, initialization from unsupervised pre-training yields substantial improvement, which does not vanish as the number of training examples increases. The online setting is also interesting because it focuses on the optimization issues rather than on the small-sample regu-larization effects, so we decided to include in our experi-ments a synthetic images dataset inspired from Larochelle et al. (2007) and Larochelle et al. (2009), from which as many examples as needed could be sampled, for testing the online learning scenario. We call this dataset the Shapeset-3 ×2 dataset, with ex-ample images in Figure 1 (top). Shapeset-3 ×2 con-tains images of 1 or 2 two-dimensional objects, each taken from 3 shape categories (triangle, parallelogram, ellipse), and placed with random shape parameters (relative lengths and/or angles), scaling, rotation, translation and grey-scale. We noticed that for only one shape present in the image the task of recognizing it was too easy. We therefore decided to sample also images with two objects, with the constraint that the second object does not overlap with the ﬁrst by more than ﬁfty percent of its area, to avoid hiding it en-tirely. The task is to predict the objects present (e.g. trian-gle + ellipse, parallelogram + parallelogram, triangle alone, etc.) without having to distinguish between the foreground shape and the background shape when they overlap. This therefore deﬁnes nine conﬁguration classes.\\n\\nPlease summarize the main points of the content.\\nAnswer:',\n",
       "  'output': 'The document discusses the challenges of training deep feedforward neural networks, particularly focusing on weight initialization and its impact on gradient propagation. Here are the main points:\\n\\n1. **Weight Initialization**: The standard initialization method leads to a variance in weights that decreases as it propagates through layers. This can negatively affect the training of deep networks.\\n\\n2. **Normalized Initialization**: The authors propose a new initialization method, termed \"normalized initialization,\" which aims to maintain consistent activation variances and back-propagated gradient variances across layers. This method uses a uniform distribution based on the sizes of the layers.\\n\\n3. **Empirical Validation**: The authors conducted experiments to validate their theoretical claims, analyzing histograms of activation values and gradients using both standard and normalized initialization methods. They observed that the normalized initialization maintains a more stable ratio of singular values in the Jacobian matrix compared to standard initialization.\\n\\n4. **Gradient Dynamics**: The document highlights the complexity of learning dynamics in deep networks. Initially, with standard initialization, the variance of back-propagated gradients decreases as they move down the layers, but this trend reverses during training. In contrast, normalized initialization does not exhibit this decreasing trend.\\n\\n5. **Weight Gradient Behavior**: Despite the decreasing back-propagated gradients with standard initialization, the variance of weight gradients remains relatively constant across layers. However, as training progresses, the gradients diverge, with lower layers exhibiting larger gradients.\\n\\nOverall, the document emphasizes the importance of weight initialization in deep networks and presents evidence that normalized initialization can lead to better gradient propagation and training dynamics.'},\n",
       " {'input': 'Human: This is a part of document:\\nfrom 3 shape categories (triangle, parallelogram, ellipse), and placed with random shape parameters (relative lengths and/or angles), scaling, rotation, translation and grey-scale. We noticed that for only one shape present in the image the task of recognizing it was too easy. We therefore decided to sample also images with two objects, with the constraint that the second object does not overlap with the ﬁrst by more than ﬁfty percent of its area, to avoid hiding it en-tirely. The task is to predict the objects present (e.g. trian-gle + ellipse, parallelogram + parallelogram, triangle alone, etc.) without having to distinguish between the foreground shape and the background shape when they overlap. This therefore deﬁnes nine conﬁguration classes. The task is fairly difﬁcult because we need to discover in-variances over rotation, translation, scaling, object color, occlusion and relative position of the shapes. In parallel we need to extract the factors of variability that predict which object shapes are present. The size of the images are arbitrary but we ﬁxed it to 32 ×32 in order to work with deep dense networks efﬁciently. 2.2 Finite Datasets The MNIST digits (LeCun et al., 1998a), dataset has 50,000 training images, 10,000 validation images (for hyper-parameter selection), and 10,000 test images, each showing a 28 ×28 grey-scale pixel image of one of the 10 digits. CIFAR-10 (Krizhevsky & Hinton, 2009) is a labelled sub-Figure 1: Top: Shapeset-3 ×2 images at 64 ×64 resolution. The examples we used are at 32 ×32 resolution. The learner tries to predict which objects (parallelogram, triangle, or el-lipse) are present, and 1 or 2 objects can be present, yield-ing 9 possible classiﬁcations. Bottom: Small-ImageNet images at full resolution. set of the tiny-images dataset that contains 50,000 training examples (from which we extracted 10,000 as validation data) and 10,000 test examples. There are 10 classes cor-responding to the main object in each image: airplane, au-tomobile, bird, cat, deer, dog, frog, horse, ship, or truck. The classes are balanced. Each image is in color, but is just 32 ×32 pixels in size, so the input is a vector of 32 ×32 ×3 = 3072real values. Small-ImageNet which is a set of tiny 37 ×37 gray level images dataset computed from the higher-resolution and larger set at http://www.image-net.org, with la-bels from the WordNet noun hierarchy. We have used 90,000 examples for training, 10,000 for the validation set, and 10,000 for testing. There are 10 balanced classes: rep-tiles, vehicles, birds, mammals, ﬁsh, furniture, instruments, tools, ﬂowers and fruits Figure 1 (bottom) shows randomly chosen examples. 2.3 Experimental Setting We optimized feedforward neural networks with one to ﬁve hidden layers, with one thousand hidden units per layer, and with a softmax logistic regression for the out-put layer. The cost function is the negative log-likelihood −log P (y|x), where (x, y) is the (input image, target class) pair. The neural networks were optimized with stochastic back-propagation on mini-batches of size ten, i.e., the av-erage g of ∂−log P (y|x) ∂θ was computed over 10 consecutive          251 Xavier Glorot, Yoshua Bengio\\n\\nPlease summarize the main points of the content.\\nAnswer:',\n",
       "  'output': 'The document discusses the effects of different weight initialization strategies and activation functions on the training of deep neural networks, particularly focusing on hyperbolic tangent (tanh) and softsign activations. Key points include:\\n\\n1. **Back-propagated Gradients**: The study observes that with standard initialization, back-propagated gradients decrease in magnitude across layers, but the variance of weight gradients remains constant. This is contrary to expectations and is explained by theoretical analysis.\\n\\n2. **Training Dynamics**: As training progresses, the gradients for standard and normalized initialization diverge, with larger gradients in lower layers for standard initialization. This divergence may lead to ill-conditioning and slower training, suggesting that normalized initialization could be advantageous.\\n\\n3. **Activation Functions**: The softsign networks exhibit similarities to tanh networks with normalized initialization, indicating that the choice of activation function and initialization method significantly impacts training dynamics.\\n\\n4. **Error Curves**: The document presents error curves that illustrate the evolution of test error during training across different datasets (Shapeset-3 ×2, MNIST, CIFAR-10, and Small-ImageNet). It highlights that a depth five hyperbolic tangent network with normalized initialization outperforms RBF SVM models on Shapeset data.\\n\\n5. **Statistical Significance**: Results are presented in a table, showing test errors for various activation functions and initialization schemes, with statistically significant differences noted for certain configurations.\\n\\n6. **Learning Rate and Saturation**: The learning rate is optimized for each network to minimize validation error. The document notes that on more difficult tasks, such as Shapeset-3 ×2, saturation effects during learning may make the benefits of normalized initialization and softsign activation more pronounced.\\n\\nOverall, the findings emphasize the importance of initialization and activation function choices in training deep networks effectively.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('corpus final count:', len(corpus_set))\n",
    "corpus_set[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [LOG] 8개 문서 중 하나 실패하였고, 총 96개의 학습 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"corpus.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(corpus_set, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
