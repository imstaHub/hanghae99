{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week8 Basic Homework\n",
    "- corpus 사용하여 facebook/opt-350m 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (4.6.4)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wandb\n",
      "  Downloading wandb-0.19.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.15.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting huggingface\n",
      "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.4.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting click (from sacremoses)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (3.11.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting pydantic<3,>=2.6 (from wandb)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.21.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.4.0)\n",
      "Collecting rich (from trl)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers)\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fsspec[http]<=2024.9.0,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions<5,>=4.4 (from wandb)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->trl)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.16.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m193.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m208.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m199.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m148.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m198.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m211.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.3.0-py3-none-any.whl (336 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.19.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m170.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.15.0-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.3/318.3 kB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m187.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m133.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 kB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 kB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.21.0-py2.py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.1/324.1 kB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: sentencepiece, pytz, huggingface, xxhash, tzdata, typing-extensions, tqdm, threadpoolctl, tabulate, smmap, setproctitle, sentry-sdk, scipy, safetensors, requests, regex, pyarrow, protobuf, propcache, portalocker, mdurl, joblib, fsspec, frozenlist, docker-pycreds, dill, colorama, click, async-timeout, annotated-types, aiohappyeyeballs, scikit-learn, sacremoses, sacrebleu, pydantic-core, pandas, multiprocess, multidict, markdown-it-py, huggingface-hub, gitdb, aiosignal, yarl, tokenizers, rich, pydantic, gitpython, accelerate, wandb, transformers, aiohttp, peft, datasets, trl, evaluate\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed accelerate-1.3.0 aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 annotated-types-0.7.0 async-timeout-5.0.1 click-8.1.8 colorama-0.4.6 datasets-3.2.0 dill-0.3.8 docker-pycreds-0.4.0 evaluate-0.4.3 frozenlist-1.5.0 fsspec-2024.9.0 gitdb-4.0.12 gitpython-3.1.44 huggingface-0.0.1 huggingface-hub-0.28.1 joblib-1.4.2 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 peft-0.14.0 portalocker-3.1.1 propcache-0.2.1 protobuf-5.29.3 pyarrow-19.0.0 pydantic-2.10.6 pydantic-core-2.27.2 pytz-2025.1 regex-2024.11.6 requests-2.32.3 rich-13.9.4 sacrebleu-2.5.1 sacremoses-0.1.1 safetensors-0.5.2 scikit-learn-1.6.1 scipy-1.15.1 sentencepiece-0.2.0 sentry-sdk-2.21.0 setproctitle-1.3.4 smmap-5.0.2 tabulate-0.9.0 threadpoolctl-3.5.0 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.48.3 trl-0.15.0 typing-extensions-4.12.2 tzdata-2025.1 wandb-0.19.6 xxhash-3.5.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install scikit-learn numpy pandas scipy tokenizers transformers datasets sacremoses sentencepiece importlib_metadata evaluate accelerate sacrebleu wandb trl peft huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import logging\n",
    "import evaluate\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "from trl import (\n",
    "    SFTConfig,\n",
    "    SFTTrainer,\n",
    "    DataCollatorForCompletionOnlyLM\n",
    ")\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#device setting\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.backends.cuda.is_built():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20022\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arr = [2, 4, 6, 8, 10]</td>\n",
       "      <td>Create an array of length 5 which contains all...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Height of triangle = opposite side length * si...</td>\n",
       "      <td>Formulate an equation to calculate the height ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def replace(self, replace_with):\\n    new_stri...</td>\n",
       "      <td>Write a replace method for a string class whic...</td>\n",
       "      <td>string = \"Hello World!\"\\nreplace_with = \"Greet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arr = [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33...</td>\n",
       "      <td>Create an array of length 15 containing number...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def find_num_distinct_states(matrix):\\n    sta...</td>\n",
       "      <td>Write a function to find the number of distinc...</td>\n",
       "      <td>matrix = [[1, 0, 0],\\n          [1, 0, 1],\\n  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              output  \\\n",
       "0                             arr = [2, 4, 6, 8, 10]   \n",
       "1  Height of triangle = opposite side length * si...   \n",
       "2  def replace(self, replace_with):\\n    new_stri...   \n",
       "3  arr = [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33...   \n",
       "4  def find_num_distinct_states(matrix):\\n    sta...   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  Create an array of length 5 which contains all...   \n",
       "1  Formulate an equation to calculate the height ...   \n",
       "2  Write a replace method for a string class whic...   \n",
       "3  Create an array of length 15 containing number...   \n",
       "4  Write a function to find the number of distinc...   \n",
       "\n",
       "                                               input  \n",
       "0                                                     \n",
       "1                                                     \n",
       "2  string = \"Hello World!\"\\nreplace_with = \"Greet...  \n",
       "3                                                     \n",
       "4  matrix = [[1, 0, 0],\\n          [1, 0, 1],\\n  ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"sahil2801/CodeAlpaca-20k\")\n",
    "\n",
    "df = data['train'].to_pandas()\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MY CODE] Model Load (opt-350m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# facebook - 2k tokens\n",
    "model_name = \"facebook/opt-350m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,563,520 || all params: 334,759,936 || trainable%: 1.0645\n"
     ]
    }
   ],
   "source": [
    "lora_r: int = 8\n",
    "lora_dropout: float = 0.1\n",
    "lora_alpha: int = 32\n",
    "\n",
    "target_modules = set()\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        names = name.split('.')\n",
    "        target_modules.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "if \"lm_head\" in target_modules:  # needed for 16-bit\n",
    "    target_modules.remove(\"lm_head\")\n",
    "\n",
    "target_modules = list(target_modules)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=target_modules\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MY CODE] 학습 준비 - training arguments, data preprocess function, collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimsta\u001b[0m (\u001b[33mimsta-hub\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250213_221052-2gstdqtz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/imsta-hub/Week8/runs/2gstdqtz' target=\"_blank\">divine-dew-7</a></strong> to <a href='https://wandb.ai/imsta-hub/Week8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/imsta-hub/Week8' target=\"_blank\">https://wandb.ai/imsta-hub/Week8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/imsta-hub/Week8/runs/2gstdqtz' target=\"_blank\">https://wandb.ai/imsta-hub/Week8/runs/2gstdqtz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb 접속\n",
    "wandb.init(project='Week8')\n",
    "wandb.run.name = 'fb-lora8-finetuning'\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(output_dir=os.path.join(os.getcwd(),'week8_model/fb_8'),\n",
    "                        learning_rate=1e-4,\n",
    "                        per_device_train_batch_size=32,\n",
    "                        per_device_eval_batch_size=32,\n",
    "                        max_seq_length=128,\n",
    "                        num_train_epochs=15,\n",
    "                        weight_decay=0.01,\n",
    "                        eval_strategy=\"epoch\",\n",
    "                        save_strategy=\"epoch\",\n",
    "                        load_best_model_at_end=True,\n",
    "                        report_to = \"wandb\",\n",
    "                        push_to_hub=False,\n",
    "                        )\n",
    "\n",
    "# logger 설정\n",
    "if training_args.should_log:\n",
    "    transformers.utils.logging.set_verbosity_info()  # log level을 INFO로 변경\n",
    "\n",
    "# log level: 10 DEBUG, 20 INFO, 30 WARNING, 40 ERROR, 50 CRITICAL\n",
    "log_level = training_args.get_process_log_level()\n",
    "\n",
    "# 우리가 가지고 있는 logger와 HuggingFace의 logger의 log level 설정\n",
    "logger.setLevel(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "\n",
    "# 기타 HuggingFace logger option들을 설정\n",
    "transformers.utils.logging.enable_default_handler() # logger 기능 활성화\n",
    "transformers.utils.logging.enable_explicit_format() # 포맷 설정: [LEVELNAME|FILENAME|LINE NUMBER] TIME >> MESSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"{example['instruction'][i]}\\n### Answer: \\n{example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Write a Python function to convert a given number of days into weeks, days.\\n### Answer: \\ndef convert_days(days):\\n  weeks = days // 7\\n  days = days % 7\\n  return (weeks, days)',\n",
       " 'Calculate the total of all the elements in the list using a loop.\\n### Answer: \\ntotal = 0\\nfor num in my_list:\\n    total += num\\nprint(total) # 20',\n",
       " 'Given a string of text, find the total number of characters in the string.\\n### Answer: \\ndef char_count(text):\\n    count = 0\\n    for char in text:\\n        count += 1\\n    return count',\n",
       " 'Suggest a modification to the following python code to make it efficient.\\n### Answer: \\ndef median(my_list):\\n\\thalf = len(my_list)//2\\n\\tmy_list.sort()\\n\\tmedian_index = len(my_list) - half\\n\\treturn my_list[median_index]\\n\\nThe suggested modification is to add a sorting step to the code before finding the median index. This will ensure the list is always in order and the median index can be found in an efficient manner.',\n",
       " 'Create an algorithm that takes two numbers, a and b, and finds their greatest common divisor (GCD).\\n### Answer: \\n// Algorithm to find the GCD of two numbers\\n\\n// Function to find GCD \\nfunction findGCD(a, b) { \\n    // If both a and b are 0 \\n    if (a == 0 && b == 0) \\n        return 0; \\n  \\n    // If a is 0, GCD is b \\n    if (a == 0) \\n        return b; \\n  \\n    // If b is 0, GCD is a \\n    if (b == 0) \\n        return a; \\n  \\n    // Find GCD recursively \\n    return findGCD(b, a % b); \\n}\\n\\n// Variable to hold a and b\\nvar a = 12;\\nvar b = 18;\\n\\n// Find GCDs\\nvar gcd = findGCD(a, b); \\n  \\n// Print GCD\\nconsole.log(\"The GCD of\", a, \"and\", b, \"is\", gcd);',\n",
       " 'Print the square of 5?\\n### Answer: \\nThe square of 5 is 25.',\n",
       " 'Write a function that takes a string as an argument, reverses the characters, and returns the reversed string.\\n### Answer: \\ndef reverse_string(string):\\n    reversed_string = \"\"\\n    for i in range(len(string)-1, -1, -1):\\n        reversed_string += string[i]\\n    return reversed_string',\n",
       " 'List the different access modifiers in Java\\n### Answer: \\nThe access modifiers in Java are public, private, protected, and default. Public members are accessible everywhere and can be inherited. Private members can only be accessed within the same class. Protected members are accessible within the same package but can be inherited to a subclass in a different package. Default, or package-private, members are accessible within the same package but cannot be inherited.',\n",
       " 'Write a CSS selector that selects all paragraphs within a div element with the class ``main-content``.\\n### Answer: \\ndiv.main-content p { /* CSS selector to select all paragraphs within a div element with the class \"main-content\" */ }']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test = data['train'].train_test_split(test_size=0.2, train_size=0.8)\n",
    "ds_train = train_test['train']\n",
    "ds_test = train_test['test']\n",
    "\n",
    "some_examples = formatting_prompts_func(ds_train[1:10])\n",
    "some_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"\\n### Answer: \"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:696] 2025-02-13 22:11:45,863 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:11:45,865 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 22:11:45,868 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 22:11:45,869 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 22:11:45,869 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 22:11:45,870 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 22:11:45,871 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 22:11:45,872 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 22:11:45,872 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:11:45,874 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:11:45,876 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:11:45,941 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:11:45,943 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|trainer.py:512] 2025-02-13 22:11:46,047 >> You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "[INFO|trainer.py:917] 2025-02-13 22:11:46,246 >> The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2369] 2025-02-13 22:11:46,275 >> ***** Running training *****\n",
      "[INFO|trainer.py:2370] 2025-02-13 22:11:46,276 >>   Num examples = 16,017\n",
      "[INFO|trainer.py:2371] 2025-02-13 22:11:46,277 >>   Num Epochs = 15\n",
      "[INFO|trainer.py:2372] 2025-02-13 22:11:46,278 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:2375] 2025-02-13 22:11:46,278 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2376] 2025-02-13 22:11:46,279 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2377] 2025-02-13 22:11:46,280 >>   Total optimization steps = 7,515\n",
      "[INFO|trainer.py:2378] 2025-02-13 22:11:46,282 >>   Number of trainable parameters = 3,563,520\n",
      "[INFO|integration_utils.py:817] 2025-02-13 22:11:46,285 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7515' max='7515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7515/7515 43:53, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.583400</td>\n",
       "      <td>1.376901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.355800</td>\n",
       "      <td>1.272009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.252500</td>\n",
       "      <td>1.213681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.178100</td>\n",
       "      <td>1.172152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.126900</td>\n",
       "      <td>1.150835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.082900</td>\n",
       "      <td>1.128005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.047700</td>\n",
       "      <td>1.116408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.017300</td>\n",
       "      <td>1.099182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.996800</td>\n",
       "      <td>1.092975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.973800</td>\n",
       "      <td>1.085662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.957900</td>\n",
       "      <td>1.082225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>1.076590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>1.072917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.920800</td>\n",
       "      <td>1.073675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.913600</td>\n",
       "      <td>1.071699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:917] 2025-02-13 22:12:58,417 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:12:58,421 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:12:58,421 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:12:58,422 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:13:04,610 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-501\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:13:05,028 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:13:05,030 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:13:05,075 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-501/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:13:05,077 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-501/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:14:23,650 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:14:23,654 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:14:23,655 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:14:23,656 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:14:36,046 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-1002\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:14:36,468 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:14:36,470 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:14:36,492 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-1002/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:14:36,493 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-1002/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:16:48,744 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:16:48,747 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:16:48,747 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:16:48,748 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:17:01,142 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-1503\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:17:01,555 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:17:01,556 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:17:01,591 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-1503/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:17:01,592 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-1503/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:19:05,485 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:19:05,487 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:19:05,488 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:19:05,488 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:19:17,946 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-2004\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:19:18,366 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:19:18,368 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:19:18,410 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-2004/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:19:18,411 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-2004/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:21:27,044 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:21:27,047 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:21:27,048 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:21:27,049 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:21:46,619 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-2505\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:21:47,052 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:21:47,054 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:21:47,091 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-2505/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:21:47,092 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-2505/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:24:51,039 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:24:51,042 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:24:51,043 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:24:51,043 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:25:10,004 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-3006\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:25:10,659 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:25:10,661 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:25:10,694 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-3006/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:25:10,695 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-3006/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:28:12,014 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:28:12,016 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:28:12,017 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:28:12,018 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:28:31,568 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-3507\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 5626e97b-7f91-4d34-9a86-701adb172a27)') - silently ignoring the lookup for the file config.json in facebook/opt-350m.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in facebook/opt-350m - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:28:41,613 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-3507/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:28:41,614 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-3507/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:31:45,511 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:31:45,513 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:31:45,514 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:31:45,515 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:32:05,144 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-4008\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:32:05,768 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:32:05,771 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:32:05,806 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-4008/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:32:05,807 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-4008/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:35:06,371 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:35:06,375 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:35:06,375 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:35:06,376 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:35:25,973 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-4509\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:35:26,380 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:35:26,382 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:35:26,425 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-4509/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:35:26,426 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-4509/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:38:28,589 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:38:28,592 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:38:28,593 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:38:28,593 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:38:47,385 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-5010\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:38:47,788 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:38:47,790 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:38:47,824 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-5010/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:38:47,825 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-5010/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:41:51,191 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:41:51,194 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:41:51,194 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:41:51,195 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:42:10,799 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-5511\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:42:11,194 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:42:11,196 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:42:11,239 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-5511/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:42:11,240 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-5511/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:45:12,010 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:45:12,013 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:45:12,014 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:45:12,014 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:45:31,689 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-6012\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:45:32,110 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:45:32,111 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:45:32,130 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-6012/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:45:32,131 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-6012/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:48:36,456 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:48:36,459 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:48:36,460 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:48:36,460 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:48:56,079 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-6513\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:48:56,483 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:48:56,484 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:48:56,507 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-6513/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:48:56,508 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-6513/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:51:57,046 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:51:57,049 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:51:57,050 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:51:57,050 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:52:16,662 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-7014\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:52:17,061 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:52:17,062 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:52:17,098 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-7014/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:52:17,099 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-7014/special_tokens_map.json\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:55:19,936 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-7515\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:55:20,355 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:55:20,356 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:55:20,381 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-7515/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:55:20,382 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-7515/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 22:55:20,461 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, instruction, text, output. If input, instruction, text, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 22:55:20,464 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 22:55:20,465 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 22:55:20,465 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:55:38,768 >> Saving model checkpoint to /workspace/week8_model/fb_8/checkpoint-7515\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:55:39,153 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:55:39,155 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:55:39,198 >> tokenizer config file saved in /workspace/week8_model/fb_8/checkpoint-7515/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:55:39,200 >> Special tokens file saved in /workspace/week8_model/fb_8/checkpoint-7515/special_tokens_map.json\n",
      "[INFO|trainer.py:2643] 2025-02-13 22:55:39,298 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2881] 2025-02-13 22:55:39,299 >> Loading best model from /workspace/week8_model/fb_8/checkpoint-7515 (score: 1.071698784828186).\n",
      "[INFO|trainer.py:3910] 2025-02-13 22:55:40,101 >> Saving model checkpoint to /workspace/week8_model/fb_8\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 22:55:40,518 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 22:55:40,520 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 22:55:40,548 >> tokenizer config file saved in /workspace/week8_model/fb_8/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 22:55:40,549 >> Special tokens file saved in /workspace/week8_model/fb_8/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  total_flos               = 52740713GF\n",
      "  train_loss               =     1.0849\n",
      "  train_runtime            = 0:43:53.81\n",
      "  train_samples_per_second =     91.219\n",
      "  train_steps_per_second   =      2.853\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=ds_train,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "    eval_dataset=ds_test\n",
    ")\n",
    "#trainer.train()\n",
    "\n",
    "checkpoint = None\n",
    "last_checkpoint = get_last_checkpoint(training_args.output_dir)  # 만약 output_dir에 checkpoint가 남아있으면 이를 사용하고, 없으면 None이 return됩니다.\n",
    "if training_args.resume_from_checkpoint is not None:  # output_dir이 아닌 다른 위치에서의 checkpoint를 resume_from_checkpoint로 지정할 수 있습니다.\n",
    "    checkpoint = training_args.resume_from_checkpoint\n",
    "else:  # 아니면 last_checkpoint로 checkpoint를 지정합니다.\n",
    "    checkpoint = last_checkpoint\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
