{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week8 Basic Homework\n",
    "- corpus 사용하여 facebook/opt-350m 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn numpy pandas scipy matplotlib tokenizers transformers datasets sacremoses sentencepiece importlib_metadata evaluate accelerate sacrebleu wandb trl peft huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import logging\n",
    "import evaluate\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "from trl import (\n",
    "    SFTConfig,\n",
    "    SFTTrainer,\n",
    "    DataCollatorForCompletionOnlyLM\n",
    ")\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#device setting\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.backends.cuda.is_built():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arr = [2, 4, 6, 8, 10]</td>\n",
       "      <td>Create an array of length 5 which contains all...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Height of triangle = opposite side length * si...</td>\n",
       "      <td>Formulate an equation to calculate the height ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def replace(self, replace_with):\\n    new_stri...</td>\n",
       "      <td>Write a replace method for a string class whic...</td>\n",
       "      <td>string = \"Hello World!\"\\nreplace_with = \"Greet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arr = [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33...</td>\n",
       "      <td>Create an array of length 15 containing number...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def find_num_distinct_states(matrix):\\n    sta...</td>\n",
       "      <td>Write a function to find the number of distinc...</td>\n",
       "      <td>matrix = [[1, 0, 0],\\n          [1, 0, 1],\\n  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              output  \\\n",
       "0                             arr = [2, 4, 6, 8, 10]   \n",
       "1  Height of triangle = opposite side length * si...   \n",
       "2  def replace(self, replace_with):\\n    new_stri...   \n",
       "3  arr = [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33...   \n",
       "4  def find_num_distinct_states(matrix):\\n    sta...   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  Create an array of length 5 which contains all...   \n",
       "1  Formulate an equation to calculate the height ...   \n",
       "2  Write a replace method for a string class whic...   \n",
       "3  Create an array of length 15 containing number...   \n",
       "4  Write a function to find the number of distinc...   \n",
       "\n",
       "                                               input  \n",
       "0                                                     \n",
       "1                                                     \n",
       "2  string = \"Hello World!\"\\nreplace_with = \"Greet...  \n",
       "3                                                     \n",
       "4  matrix = [[1, 0, 0],\\n          [1, 0, 1],\\n  ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"sahil2801/CodeAlpaca-20k\")\n",
    "\n",
    "df = data['train'].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MY CODE] Model Load (opt-350m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# facebook - 2k tokens\n",
    "model_name = \"facebook/opt-350m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 57,016,320 || all params: 388,212,736 || trainable%: 14.6869\n"
     ]
    }
   ],
   "source": [
    "lora_r: int = 128\n",
    "lora_dropout: float = 0.1\n",
    "lora_alpha: int = 32\n",
    "\n",
    "target_modules = set()\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        names = name.split('.')\n",
    "        target_modules.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "if \"lm_head\" in target_modules:  # needed for 16-bit\n",
    "    target_modules.remove(\"lm_head\")\n",
    "\n",
    "target_modules = list(target_modules)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=target_modules\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MY CODE] 학습 준비 - training arguments, data preprocess function, collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimsta\u001b[0m (\u001b[33mimsta-hub\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250213_232352-yxhu6kqb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/imsta-hub/Week8/runs/yxhu6kqb' target=\"_blank\">bumbling-glade-18</a></strong> to <a href='https://wandb.ai/imsta-hub/Week8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/imsta-hub/Week8' target=\"_blank\">https://wandb.ai/imsta-hub/Week8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/imsta-hub/Week8/runs/yxhu6kqb' target=\"_blank\">https://wandb.ai/imsta-hub/Week8/runs/yxhu6kqb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb 접속\n",
    "wandb.init(project='Week8')\n",
    "wandb.run.name = 'fb-lora128-finetuning'\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(output_dir=os.path.join(os.getcwd(),'week8_model/fb_128'),\n",
    "                        learning_rate=1e-4,\n",
    "                        per_device_train_batch_size=32,\n",
    "                        per_device_eval_batch_size=32,\n",
    "                        max_seq_length=128,\n",
    "                        num_train_epochs=15,\n",
    "                        weight_decay=0.01,\n",
    "                        eval_strategy=\"epoch\",\n",
    "                        save_strategy=\"epoch\",\n",
    "                        load_best_model_at_end=True,\n",
    "                        report_to = \"wandb\",\n",
    "                        push_to_hub=False,\n",
    "                        )\n",
    "\n",
    "# logger 설정\n",
    "if training_args.should_log:\n",
    "    transformers.utils.logging.set_verbosity_info()  # log level을 INFO로 변경\n",
    "\n",
    "# log level: 10 DEBUG, 20 INFO, 30 WARNING, 40 ERROR, 50 CRITICAL\n",
    "log_level = training_args.get_process_log_level()\n",
    "\n",
    "# 우리가 가지고 있는 logger와 HuggingFace의 logger의 log level 설정\n",
    "logger.setLevel(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "\n",
    "# 기타 HuggingFace logger option들을 설정\n",
    "transformers.utils.logging.enable_default_handler() # logger 기능 활성화\n",
    "transformers.utils.logging.enable_explicit_format() # 포맷 설정: [LEVELNAME|FILENAME|LINE NUMBER] TIME >> MESSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"{example['instruction'][i]}\\n### Answer: \\n{example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = data['train'].train_test_split(test_size=0.2, train_size=0.8)\n",
    "ds_train = train_test['train']\n",
    "ds_test = train_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"\\n### Answer: \"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:696] 2025-02-13 23:24:24,670 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:24:24,672 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 23:24:24,675 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 23:24:24,675 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 23:24:24,676 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 23:24:24,677 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 23:24:24,678 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 23:24:24,678 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-13 23:24:24,679 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 23:24:24,680 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:24:24,682 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 23:24:24,749 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:24:24,750 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0b81a4095e46eaa9380d1abcaed585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/16017 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ffc63c877941c3ab9a14592f90115d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/16017 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1354932163e54cbdb412dc5fcf68ce44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/16017 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7a4bc2e3aa4a1484deefb74fe70eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/16017 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b405e0ad474faea3312312b8cc34c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/4005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c4ba21865145eead0441fe7df93f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/4005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9a73a7ba6f4d7698905f0792e5f888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/4005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb61b4f6634245149595c41bb31d06bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/4005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:512] 2025-02-13 23:24:31,802 >> You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "[INFO|trainer.py:917] 2025-02-13 23:24:31,988 >> The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2369] 2025-02-13 23:24:32,018 >> ***** Running training *****\n",
      "[INFO|trainer.py:2370] 2025-02-13 23:24:32,019 >>   Num examples = 16,017\n",
      "[INFO|trainer.py:2371] 2025-02-13 23:24:32,020 >>   Num Epochs = 15\n",
      "[INFO|trainer.py:2372] 2025-02-13 23:24:32,021 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:2375] 2025-02-13 23:24:32,021 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2376] 2025-02-13 23:24:32,023 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2377] 2025-02-13 23:24:32,023 >>   Total optimization steps = 7,515\n",
      "[INFO|trainer.py:2378] 2025-02-13 23:24:32,027 >>   Number of trainable parameters = 57,016,320\n",
      "[INFO|integration_utils.py:817] 2025-02-13 23:24:32,031 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7150' max='7515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7150/7515 43:27 < 02:13, 2.74 it/s, Epoch 14.27/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.570900</td>\n",
       "      <td>1.337613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.329900</td>\n",
       "      <td>1.232667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.218800</td>\n",
       "      <td>1.168351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.143400</td>\n",
       "      <td>1.127570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.084600</td>\n",
       "      <td>1.100392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.037700</td>\n",
       "      <td>1.078341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.000300</td>\n",
       "      <td>1.064890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.965100</td>\n",
       "      <td>1.053825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.939300</td>\n",
       "      <td>1.045502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.915300</td>\n",
       "      <td>1.038271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.891300</td>\n",
       "      <td>1.030460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.879200</td>\n",
       "      <td>1.027935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.861100</td>\n",
       "      <td>1.026032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.854300</td>\n",
       "      <td>1.023300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:917] 2025-02-13 23:27:28,541 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 23:27:28,544 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 23:27:28,545 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 23:27:28,545 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 23:27:47,633 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-501\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 23:27:48,051 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:27:48,052 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 23:27:48,220 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-501/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 23:27:48,221 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-501/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 23:30:44,888 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 23:30:44,890 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 23:30:44,891 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 23:30:44,891 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 23:31:04,030 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-1002\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 23:31:04,442 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:31:04,443 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 23:31:04,619 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-1002/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 23:31:04,620 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-1002/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 23:33:08,403 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 23:33:08,406 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 23:33:08,407 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 23:33:08,408 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 23:33:16,421 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-1503\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 23:33:16,857 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:33:16,858 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 23:33:17,034 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-1503/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 23:33:17,035 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-1503/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 23:34:45,390 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 23:34:45,392 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 23:34:45,393 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 23:34:45,393 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 23:34:53,254 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-2004\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 23:34:53,652 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:34:53,653 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 23:34:53,816 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-2004/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 23:34:53,817 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-2004/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 23:37:22,775 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 23:37:22,778 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 23:37:22,779 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 23:37:22,779 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 23:37:41,791 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-2505\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 23:37:42,185 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:37:42,187 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 23:37:42,353 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-2505/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 23:37:42,354 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-2505/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 23:40:38,175 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 23:40:38,178 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 23:40:38,179 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 23:40:38,179 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 23:40:57,296 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-3006\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 23:40:57,703 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:40:57,704 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 23:40:57,873 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-3006/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 23:40:57,874 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-3006/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 23:43:53,643 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 23:43:53,646 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 23:43:53,647 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 23:43:53,647 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 23:44:12,900 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-3507\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 23:44:13,301 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:44:13,303 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 23:44:13,468 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-3507/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 23:44:13,469 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-3507/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 23:47:10,434 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 23:47:10,437 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 23:47:10,438 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 23:47:10,438 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 23:47:29,434 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-4008\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 23:47:29,842 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:47:29,844 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 23:47:30,013 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-4008/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 23:47:30,014 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-4008/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 23:50:28,617 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 23:50:28,620 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 23:50:28,621 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 23:50:28,621 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 23:50:45,829 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-4509\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 23:50:46,234 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:50:46,236 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 23:50:46,397 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-4509/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 23:50:46,398 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-4509/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 23:53:46,255 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 23:53:46,258 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 23:53:46,258 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 23:53:46,259 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 23:54:05,314 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-5010\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 23:54:05,713 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:54:05,714 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 23:54:05,881 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-5010/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 23:54:05,882 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-5010/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-13 23:57:03,006 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-13 23:57:03,009 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-13 23:57:03,009 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-13 23:57:03,010 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-13 23:57:22,255 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-5511\n",
      "[INFO|configuration_utils.py:696] 2025-02-13 23:57:22,652 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-13 23:57:22,653 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-13 23:57:22,794 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-5511/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-13 23:57:22,795 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-5511/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-14 00:00:19,602 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-14 00:00:19,605 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-14 00:00:19,606 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-14 00:00:19,606 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-14 00:00:38,363 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-6012\n",
      "[INFO|configuration_utils.py:696] 2025-02-14 00:00:38,762 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-14 00:00:38,763 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-14 00:00:38,929 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-6012/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-14 00:00:38,930 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-6012/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-14 00:03:35,650 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-14 00:03:35,652 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-14 00:03:35,653 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-14 00:03:35,653 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-14 00:03:54,494 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-6513\n",
      "[INFO|configuration_utils.py:696] 2025-02-14 00:03:54,890 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-14 00:03:54,892 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-14 00:03:55,067 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-6513/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-14 00:03:55,069 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-6513/special_tokens_map.json\n",
      "[INFO|trainer.py:917] 2025-02-14 00:06:51,834 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: instruction, output, input, text. If instruction, output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-02-14 00:06:51,836 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-02-14 00:06:51,837 >>   Num examples = 4005\n",
      "[INFO|trainer.py:4231] 2025-02-14 00:06:51,837 >>   Batch size = 32\n",
      "[INFO|trainer.py:3910] 2025-02-14 00:07:10,981 >> Saving model checkpoint to /workspace/week8_model/fb_128/checkpoint-7014\n",
      "[INFO|configuration_utils.py:696] 2025-02-14 00:07:11,390 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-14 00:07:11,392 >> Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-14 00:07:11,611 >> tokenizer config file saved in /workspace/week8_model/fb_128/checkpoint-7014/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-14 00:07:11,612 >> Special tokens file saved in /workspace/week8_model/fb_128/checkpoint-7014/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=ds_train,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "    eval_dataset=ds_test\n",
    ")\n",
    "#trainer.train()\n",
    "\n",
    "# checkpoint = None\n",
    "# last_checkpoint = get_last_checkpoint(training_args.output_dir)  # 만약 output_dir에 checkpoint가 남아있으면 이를 사용하고, 없으면 None이 return됩니다.\n",
    "# if training_args.resume_from_checkpoint is not None:  # output_dir이 아닌 다른 위치에서의 checkpoint를 resume_from_checkpoint로 지정할 수 있습니다.\n",
    "#     checkpoint = training_args.resume_from_checkpoint\n",
    "# else:  # 아니면 last_checkpoint로 checkpoint를 지정합니다.\n",
    "#     checkpoint = last_checkpoint\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
