{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week8 Basic Homework\n",
    "- ChatGPT의 MapReduce를 모방한 요약 corpus 만들기\n",
    "- input data size 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, List, TypedDict\n",
    "\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "## MapReduce class and function\n",
    "class OverallState(TypedDict):\n",
    "    # Notice here we use the operator.add\n",
    "    # This is because we want combine all the summaries we generate\n",
    "    # from individual nodes back into one list - this is essentially\n",
    "    # the \"reduce\" part\n",
    "    contents: List[str]\n",
    "    summaries: Annotated[list, operator.add]\n",
    "    final_summary: str\n",
    "\n",
    "\n",
    "# This will be the state of the node that we will \"map\" all\n",
    "# documents to in order to generate summaries\n",
    "class SummaryState(TypedDict):\n",
    "    content: str\n",
    "\n",
    "\n",
    "# Here we generate a summary, given a document\n",
    "async def generate_summary(state: SummaryState):\n",
    "    response = await map_chain.ainvoke(state[\"content\"])\n",
    "    return {\"summaries\": [response]}\n",
    "\n",
    "\n",
    "# Here we define the logic to map out over the documents\n",
    "# We will use this an edge in the graph\n",
    "def map_summaries(state: OverallState):\n",
    "    # We will return a list of `Send` objects\n",
    "    # Each `Send` object consists of the name of a node in the graph\n",
    "    # as well as the state to send to that node\n",
    "    return [\n",
    "        Send(\"generate_summary\", {\"content\": content}) for content in state[\"contents\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "# Here we will generate the final summary\n",
    "async def generate_final_summary(state: OverallState):\n",
    "    response = await reduce_chain.ainvoke(state[\"summaries\"])\n",
    "    return {\"final_summary\": response}\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pprint\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MY CODE] 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dl paper.pdf', 'Artical.pdf', 'deep neural network.pdf', 'bayesian deep learning.pdf', 'RAG_paper.pdf', 'An Improved Particle Filter.pdf', 'NIPS-2017-attention.pdf', 'deep learning.pdf']\n"
     ]
    }
   ],
   "source": [
    "file_list = os.listdir('week7_pdf')\n",
    "file_list = [x for x in file_list if x not in ['.DS_Store']]\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map prompt\n",
    "map_template = \"\"\"This is a part of document:\n",
    "{pages}\n",
    "\n",
    "Please summarize the main points of the content.\n",
    "Answer:\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# Reduce prompt\n",
    "reduce_template = \"\"\"This is a set of summary:\n",
    "{doc_summaries}\n",
    "\n",
    "Please write a comprehensive summary of this..\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Reduce 프롬프트 완성\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0,\n",
    "                 model_name='gpt-4o-mini',\n",
    "                 api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 2 65536 (offset 0)\n",
      "Ignoring wrong pointing object 34 65536 (offset 0)\n",
      "Ignoring wrong pointing object 92 65536 (offset 0)\n",
      "Ignoring wrong pointing object 145 65536 (offset 0)\n",
      "Ignoring wrong pointing object 206 65536 (offset 0)\n",
      "Ignoring wrong pointing object 274 65536 (offset 0)\n",
      "Ignoring wrong pointing object 330 65536 (offset 0)\n",
      "Ignoring wrong pointing object 372 65536 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl paper.pdf | doc 수: 8\n",
      "dl paper.pdf | split 수: 3\n",
      "corpus count: 4\n",
      "dl paper.pdf | split 수: 3\n",
      "corpus count: 8\n",
      "dl paper.pdf | split 수: 3\n",
      "corpus count: 12\n",
      "dl paper.pdf | split 수: 3\n",
      "corpus count: 16\n",
      "Artical.pdf | doc 수: 16\n",
      "Artical.pdf | split 수: 3\n",
      "corpus count: 20\n",
      "Artical.pdf | split 수: 3\n",
      "corpus count: 24\n",
      "Artical.pdf | split 수: 3\n",
      "corpus count: 28\n",
      "Artical.pdf | split 수: 3\n",
      "corpus count: 32\n",
      "Artical.pdf | split 수: 2\n",
      "corpus count: 35\n",
      "Artical.pdf | split 수: 3\n",
      "corpus count: 39\n",
      "Artical.pdf | split 수: 3\n",
      "corpus count: 43\n",
      "deep neural network.pdf | doc 수: 9\n",
      "deep neural network.pdf | split 수: 3\n",
      "corpus count: 47\n",
      "deep neural network.pdf | split 수: 3\n",
      "corpus count: 51\n",
      "deep neural network.pdf | split 수: 3\n",
      "corpus count: 55\n",
      "deep neural network.pdf | split 수: 2\n",
      "corpus count: 58\n",
      "bayesian deep learning.pdf | doc 수: 7\n",
      "bayesian deep learning.pdf | split 수: 3\n",
      "corpus count: 62\n",
      "RAG_paper.pdf | doc 수: 19\n",
      "RAG_paper.pdf | split 수: 3\n",
      "corpus count: 66\n",
      "RAG_paper.pdf | split 수: 3\n",
      "corpus count: 70\n",
      "RAG_paper.pdf | split 수: 3\n",
      "corpus count: 74\n",
      "RAG_paper.pdf | split 수: 3\n",
      "corpus count: 78\n",
      "RAG_paper.pdf | split 수: 3\n",
      "corpus count: 82\n",
      "An Improved Particle Filter.pdf | doc 수: 14\n",
      "An Improved Particle Filter.pdf | split 수: 3\n",
      "corpus count: 86\n",
      "An Improved Particle Filter.pdf | split 수: 3\n",
      "corpus count: 90\n",
      "An Improved Particle Filter.pdf | split 수: 3\n",
      "corpus count: 94\n",
      "An Improved Particle Filter.pdf | split 수: 3\n",
      "corpus count: 98\n",
      "An Improved Particle Filter.pdf | split 수: 3\n",
      "corpus count: 102\n",
      "NIPS-2017-attention.pdf | doc 수: 11\n",
      "NIPS-2017-attention.pdf | split 수: 3\n",
      "corpus count: 106\n",
      "NIPS-2017-attention.pdf | split 수: 3\n",
      "corpus count: 110\n",
      "NIPS-2017-attention.pdf | split 수: 3\n",
      "corpus count: 114\n",
      "NIPS-2017-attention.pdf | split 수: 3\n",
      "corpus count: 118\n",
      "NIPS-2017-attention.pdf | split 수: 2\n",
      "corpus count: 121\n",
      "deep learning.pdf | doc 수: 11\n",
      "deep learning.pdf | split 수: 3\n",
      "corpus count: 125\n",
      "deep learning.pdf | split 수: 3\n",
      "corpus count: 129\n",
      "deep learning.pdf | split 수: 3\n",
      "corpus count: 133\n",
      "deep learning.pdf | split 수: 3\n",
      "corpus count: 137\n",
      "deep learning.pdf | split 수: 2\n",
      "corpus count: 140\n"
     ]
    }
   ],
   "source": [
    "corpus_set = []\n",
    "for file_name in file_list:\n",
    "    try:\n",
    "        loader = PyPDFLoader(\n",
    "            file_path = 'week7_pdf/'+file_name,\n",
    "        )\n",
    "        docs = loader.load()\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        tmp = doc.page_content\n",
    "        if('References' in tmp):\n",
    "            tmp = tmp.split('References\\n')[0]\n",
    "            doc.page_content = tmp\n",
    "            new_docs.append(doc)\n",
    "            break\n",
    "        elif('references' in tmp):\n",
    "            tmp = tmp.split('references\\n')[0]\n",
    "            doc.page_content = tmp\n",
    "            new_docs.append(doc)\n",
    "            break\n",
    "        else:\n",
    "            new_docs.append(doc)\n",
    "\n",
    "    ## 3 page씩 요약\n",
    "    print(file_name, '| doc 수:', len(docs))\n",
    "    base_page = 2\n",
    "    for i in range(int(len(new_docs)/base_page)):\n",
    "        if(i+1==int(len(new_docs)/base_page)):\n",
    "            docs = new_docs[i*base_page:]\n",
    "        else:\n",
    "            docs = new_docs[i*base_page:(i+1)*base_page]\n",
    "\n",
    "        page_contents = [doc.page_content for doc in docs]\n",
    "\n",
    "        page_contents = '\\n'.join(page_contents)\n",
    "\n",
    "        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "            separator=\"\\n\",  # 분할기준\n",
    "            chunk_size=500,   # 사이즈\n",
    "            chunk_overlap=50, # 중첩 사이즈\n",
    "        )\n",
    "\n",
    "        # 분할 실행\n",
    "        split_docs = text_splitter.split_text(page_contents)\n",
    "        split_docs = [split.replace('-\\n','-').replace('\\n',' ') for split in split_docs]\n",
    "        split_docs = split_docs[:3]\n",
    "\n",
    "        print(file_name, '| split 수:', len(split_docs))\n",
    "\n",
    "\n",
    "        ### Map Reduce\n",
    "        map_prompt = ChatPromptTemplate([(\"human\", map_template)])\n",
    "        map_chain = map_prompt | llm | StrOutputParser()\n",
    "\n",
    "        reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
    "        reduce_chain = reduce_prompt | llm | StrOutputParser()\n",
    "\n",
    "        # Construct the graph: here we put everything together to construct our graph\n",
    "        graph = StateGraph(OverallState)\n",
    "        graph.add_node(\"generate_summary\", generate_summary)\n",
    "        graph.add_node(\"generate_final_summary\", generate_final_summary)\n",
    "        graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])\n",
    "        graph.add_edge(\"generate_summary\", \"generate_final_summary\")\n",
    "        graph.add_edge(\"generate_final_summary\", END)\n",
    "        app = graph.compile()\n",
    "\n",
    "        rsts = []\n",
    "        async for step in app.astream({\"contents\": split_docs}):\n",
    "            rsts.append(step)\n",
    "            #print(list(step.keys()))\n",
    "\n",
    "\n",
    "        # GPT 결과와 map prompt, reduce prompt를 통해 corpus 데이터셋 만들기\n",
    "        for i in range(len(rsts)):\n",
    "            if('generate_summary' in rsts[i].keys()):\n",
    "                map_content = rsts[i]['generate_summary']['summaries']\n",
    "                if(not isinstance(map,str)):\n",
    "                    map_content = map_content[0]\n",
    "                corpus = {\"input\": map_prompt.format(pages=split_docs[i]),\n",
    "                        \"output\": map_content}\n",
    "\n",
    "\n",
    "            else:\n",
    "                maps = [rst['generate_summary']['summaries'][0] for rst in rsts[:-1]]\n",
    "                maps_content = '\\n'.join(maps)\n",
    "                reduce_content = rsts[i]['generate_final_summary']['final_summary']\n",
    "                corpus = {\"input\": reduce_prompt.format(doc_summaries=maps_content),\n",
    "                        \"output\": reduce_content}\n",
    "\n",
    "            corpus_set.append(corpus)\n",
    "\n",
    "        print('corpus count:', len(corpus_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 138\n"
     ]
    }
   ],
   "source": [
    "# 총 글자수가 8100개 미만으로 유지\n",
    "tmp_corpus_set = []\n",
    "for c in corpus_set:\n",
    "    if(len(c['input'])+len(c['output'])<8100):\n",
    "        tmp_corpus_set.append(c)\n",
    "\n",
    "print(len(corpus_set), len(tmp_corpus_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_set = tmp_corpus_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"corpus_small.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(corpus_set, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
