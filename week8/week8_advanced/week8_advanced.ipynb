{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week8 Advanced Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "#device setting\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.backends.cuda.is_built():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "model_name1 = os.path.join(base_path,'trained_model/fb_8')\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name1,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_path = os.getcwd()\n",
    "model_name2 = os.path.join(base_path,'trained_model/fb_128')\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name2,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_path = os.getcwd()\n",
    "model_name3 = os.path.join(base_path,'trained_model/fb_256')\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(model_name3)\n",
    "model3 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name3,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus_small.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MY CODE] 모델 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_i = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT 4.0 요약\n",
      "The documents collectively address the challenges and advancements in training deep feedforward neural networks, particularly since 2006. They highlight several key themes and findings:\n",
      "\n",
      "1. **Historical Context and Training Challenges**: Before 2006, deep multi-layer neural networks struggled with effective training. Recent algorithms have shown that these deep architectures can outperform shallower networks. The authors investigate why standard gradient descent from random initialization is often ineffective for deep networks, leading to the development of new training mechanisms.\n",
      "\n",
      "2. **Activation Functions**: The logistic sigmoid activation function is criticized for being unsuitable for deep networks due to its tendency to saturate, which can impede training. The authors suggest that using alternative non-linearities that saturate less may enhance training outcomes.\n",
      "\n",
      "3. **Layer Dynamics and Initialization**: The study explores how activations and gradients behave across layers during training, proposing that significant deviations in the singular values of the Jacobian can complicate the training process. A new initialization scheme is proposed, which is shown to facilitate faster convergence in training deep networks.\n",
      "\n",
      "4. **Advancements in Learning Methods**: The documents also discuss the broader advancements in learning methods for deep architectures, including neural networks and graphical models. They emphasize the theoretical and empirical success of these methods in complex fields like vision and natural language processing (NLP). \n",
      "\n",
      "5. **Unsupervised Pre-training vs. Supervised Methods**: A significant point made is the effectiveness of unsupervised pre-training, which helps in initializing parameters favorably within the optimization landscape, leading to better generalization. However, the limitations of unsupervised pre-training compared to purely supervised methods are also noted. The authors reference earlier work that demonstrated the benefits of a greedy layer-wise training approach.\n",
      "\n",
      "6. **Experimental Insights**: The experimental setting involves online learning with a synthetic dataset, Shapeset-3 ×2, which allows for the generation of an infinite number of examples. The findings indicate that unsupervised pre-training can still be beneficial, even with large training sets, and that these benefits persist as the number of training examples increases.\n",
      "\n",
      "Overall, the documents underscore the importance of understanding activation functions, initialization strategies, and the role of different training methods in improving the performance of deep learning models.\n"
     ]
    }
   ],
   "source": [
    "print(\"ChatGPT 4.0 요약\")\n",
    "print(corpus[num_i]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The document provides an in-depth analysis of the challenges of training deep neural networks, particularly focusing\n"
     ]
    }
   ],
   "source": [
    "# lora8\n",
    "input_text = corpus[num_i]['input']\n",
    "input_ids = tokenizer1(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model1.generate(**input_ids)\n",
    "ouput_content = tokenizer1.decode(outputs[0])\n",
    "ouput_content = ouput_content.replace('</s>'+input_text,'')\n",
    "print(ouput_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The document provides an in-depth analysis of the challenges of training deep neural networks, particularly focusing\n"
     ]
    }
   ],
   "source": [
    "# lora128\n",
    "input_text = corpus[num_i]['input']\n",
    "input_ids = tokenizer2(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model2.generate(**input_ids)\n",
    "ouput_content = tokenizer2.decode(outputs[0])\n",
    "ouput_content = ouput_content.replace('</s>'+input_text,'')\n",
    "print(ouput_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The document provides an in-depth analysis of advancements in learning methods for deep architectures, particularly neural\n"
     ]
    }
   ],
   "source": [
    "# lora256\n",
    "input_text = corpus[num_i]['input']\n",
    "input_ids = tokenizer3(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model3.generate(**input_ids)\n",
    "ouput_content = tokenizer3.decode(outputs[0])\n",
    "ouput_content = ouput_content.replace('</s>'+input_text,'')\n",
    "print(ouput_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [LOG] LoRA 학습이 잘 되지 않았음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
